import logging
import fitz  # PyMuPDF for faster text extraction
from langdetect import detect, LangDetectException
from typing import Optional


def map_lang_to_tesseract(lang_code: str) -> str:
    """Maps a language code from langdetect to a Tesseract language code."""
    mapping = {
        "zh-cn": "chi_sim",
        "zh": "chi_sim",  # Default Chinese to simplified
        "zh-tw": "chi_tra", 
        "ja": "jpn",
        "ko": "kor",
        "de": "deu",
        "fr": "fra",
        "es": "spa",
        "ru": "rus",
        "ar": "ara",
        "hi": "hin",
        "it": "ita",
        "pt": "por",
        # Add more mappings as needed
    }
    return mapping.get(lang_code, lang_code)


def detect_language_from_first_text_page(pdf_path: str) -> Optional[str]:
    """Detect language from the first page with text using PyMuPDF."""
    print("ðŸ” Detecting language from PDF...")
    try:
        doc = fitz.open(pdf_path)
        first_page_text = ""
        
        # Find first page with substantial text
        for page_num in range(min(3, doc.page_count)):  # Check first 3 pages max
            page = doc[page_num]
            text = page.get_text().strip()
            if text and len(text) > 50:  # Need substantial text for reliable detection
                first_page_text = text
                print(f"ðŸ“„ Using page {page_num + 1} for language detection")
                break
        
        doc.close()
        
        if not first_page_text:
            print("âš ï¸ No substantial text found for language detection.")
            return None

        # Detect language with priority: English â†’ Simplified Chinese â†’ Traditional Chinese
        detected_lang_code = detect(first_page_text)
        tesseract_lang = map_lang_to_tesseract(detected_lang_code)
        
        # Priority handling for Chinese
        if detected_lang_code.startswith('zh'):
            # Try to distinguish Traditional vs Simplified by character analysis
            traditional_chars = set('ç¹é«”ä¸­æ–‡å‚³çµ±å­—å½¢è¯èªžåœ‹èªžè‡ºç£é¦™æ¸¯æ¾³é–€')
            simplified_chars = set('ç®€ä½“ä¸­æ–‡ä¼ ç»Ÿå­—å½¢åŽè¯­å›½è¯­å°æ¹¾é¦™æ¸¯æ¾³é—¨')
            
            traditional_count = sum(1 for char in first_page_text if char in traditional_chars)
            simplified_count = sum(1 for char in first_page_text if char in simplified_chars)
            
            if traditional_count > simplified_count:
                tesseract_lang = "chi_tra"
                print(f"âœ… Chinese detected as Traditional: {detected_lang_code} â†’ {tesseract_lang}")
            else:
                tesseract_lang = "chi_sim"
                print(f"âœ… Chinese detected as Simplified: {detected_lang_code} â†’ {tesseract_lang}")
        else:
            print(f"âœ… Language detected: {detected_lang_code} â†’ {tesseract_lang}")
        
        return tesseract_lang

    except LangDetectException:
        print("âš ï¸ Language detection failed.")
        return None
    except Exception as e:
        print(f"âŒ Error during language detection: {e}")
        return None


def get_ocr_language_string(detected_lang: Optional[str] = None) -> str:
    """Get OCR language string with English and Simplified Chinese as base."""
    base_langs = "eng+chi_sim"
    
    if detected_lang and detected_lang not in ["eng", "chi_sim"]:
        return f"{base_langs}+{detected_lang}"
    else:
        return base_langs


def get_vertical_ocr_languages() -> str:
    """Get OCR languages for vertical text processing."""
    return "chi_tra+jpn"


def get_auto_mode_horizontal_ocr_languages() -> str:
    """Get OCR languages for horizontal pages in auto mode."""
    return "chi_tra+jpn"
