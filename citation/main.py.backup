import os
import subprocess
import logging
import json
import yaml
import fitz  # PyMuPDF
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil.parser import parse as date_parse
import dspy
from typing import Dict, Optional, List, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(levelname)s - %(message)s"
)

class CitationExtractor:
    def __init__(self, llm_model="ollama/qwen3"):
        """Initialize the citation extractor."""
        # Initialize DSPy with Ollama
        import dspy
        self.llm = dspy.LM(model=llm_model, base_url="http://localhost:11434")
        dspy.settings.configure(lm=self.llm)
        
    def extract_from_pdf(self, input_pdf_path: str, output_dir: str = "citations") -> Optional[Dict]:
        """Main function to extract citation from PDF following the workflow."""
        try:
            logging.info(f"Starting PDF citation extraction for: {input_pdf_path}")
            
            # Step 1: Check PDF page count and extract metadata
            num_pages, filename = self._analyze_pdf_structure(input_pdf_path)
            doc_type = self._determine_document_type(num_pages)
            
            logging.info(f"Document type: {doc_type}, Pages: {num_pages}")
            
            # Step 2: Try refextract for metadata
            citation_info = self._extract_with_refextract(input_pdf_path)
            
            # Step 3: Try scholarly search if we have title
            if citation_info and citation_info.get('title'):
                scholarly_info = self._search_with_scholarly(citation_info['title'])
                if scholarly_info:
                    citation_info.update(scholarly_info)
            
            # Step 4: Check if we have enough required info
            if not self._has_required_info(citation_info, doc_type):
                logging.info("Insufficient metadata found, proceeding with OCR and LLM extraction")
                
                # Step 4: OCR if needed
                searchable_pdf = self._ensure_searchable_pdf(input_pdf_path, num_pages)
                
                # Step 5: LLM extraction
                citation_info = self._extract_with_llm(searchable_pdf, doc_type)
            
            if citation_info:
                # Step 6: Save output
                self._save_citation(citation_info, input_pdf_path, output_dir)
                logging.info(f"Citation extraction completed successfully")
                return citation_info
            else:
                logging.error("Failed to extract citation information")
                return None
                
        except Exception as e:
            logging.error(f"Error extracting citation from PDF: {e}")
            return None
    
    def extract_from_url(self, url: str, output_dir: str = "citations") -> Optional[Dict]:
        """Extract citation from URL."""
        try:
            logging.info(f"Starting URL citation extraction for: {url}")
            
            # Step 1: Determine URL type
            url_type = self._determine_url_type(url)
            
            # Step 2: Extract using anystyle for text-based content
            if url_type == "text":
                citation_info = self._extract_url_with_anystyle(url)
            else:
                # Step 3: Extract metadata for video/audio
                citation_info = self._extract_media_metadata(url)
            
            if citation_info:
                citation_info['url'] = url
                citation_info['date_accessed'] = datetime.now().strftime('%Y-%m-%d')
                self._save_citation(citation_info, url, output_dir)
                logging.info(f"URL citation extraction completed successfully")
                return citation_info
            else:
                logging.error("Failed to extract citation from URL")
                return None
                
        except Exception as e:
            logging.error(f"Error extracting citation from URL: {e}")
            return None
    
    def _analyze_pdf_structure(self, pdf_path: str) -> tuple:
        """Analyze PDF structure using PyMuPDF."""
        try:
            doc = fitz.open(pdf_path)
            num_pages = doc.page_count
            filename = os.path.basename(pdf_path)
            
            # Extract basic metadata
            metadata = doc.metadata
            logging.info(f"PDF metadata: {metadata}")
            
            doc.close()
            return num_pages, filename
        except Exception as e:
            logging.error(f"Error analyzing PDF structure: {e}")
            return 0, ""
    
    def _determine_document_type(self, num_pages: int) -> str:
        """Determine document type based on page count."""
        if num_pages > 50:
            return "book_or_thesis"
        else:
            return "journal_or_chapter"
    
    def _extract_with_refextract(self, pdf_path: str) -> Dict:
        """Extract metadata using refextract."""
        try:
            # Use subprocess to call refextract
            result = subprocess.run(
                ['refextract', pdf_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                # Parse refextract output
                # Note: This is a simplified parsing - you may need to adjust based on actual output
                metadata = {}
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'title:' in line.lower():
                        metadata['title'] = line.split(':', 1)[1].strip()
                    elif 'author:' in line.lower():
                        metadata['author'] = line.split(':', 1)[1].strip()
                    elif 'year:' in line.lower():
                        metadata['year'] = line.split(':', 1)[1].strip()
                
                logging.info(f"Refextract metadata: {metadata}")
                return metadata
            else:
                logging.warning(f"Refextract failed: {result.stderr}")
                return {}
        except Exception as e:
            logging.error(f"Error with refextract: {e}")
            return {}
    
    def _search_with_scholarly(self, title: str) -> Dict:
        """Search Google Scholar using scholarly library."""
        try:
            import scholarly
            
            # Search for the publication
            search_query = scholarly.search_pubs(title)
            pub = next(search_query, None)
            
            if pub:
                # Fill in publication details
                filled_pub = scholarly.fill(pub)
                
                # Extract relevant information
                info = {}
                if 'title' in filled_pub:
                    info['title'] = filled_pub['title']
                if 'author' in filled_pub:
                    authors = [author['name'] for author in filled_pub['author']]
                    info['author'] = ', '.join(authors)
                if 'year' in filled_pub:
                    info['year'] = filled_pub['year']
                if 'venue' in filled_pub:
                    info['journal_name'] = filled_pub['venue']
                if 'publisher' in filled_pub:
                    info['publisher'] = filled_pub['publisher']
                
                logging.info(f"Scholarly info: {info}")
                return info
            else:
                logging.warning("No results found in Google Scholar")
                return {}
        except Exception as e:
            logging.error(f"Error with scholarly search: {e}")
            return {}
    
    def _has_required_info(self, citation_info: Dict, doc_type: str) -> bool:
        """Check if we have enough required information."""
        if not citation_info:
            return False
        
        required_fields = ['title', 'author']
        
        if doc_type == "book_or_thesis":
            required_fields.extend(['publisher', 'year'])
        else:
            required_fields.extend(['journal_name', 'year'])
        
        for field in required_fields:
            if not citation_info.get(field):
                return False
        
        return True
    
    def _ensure_searchable_pdf(self, pdf_path: str, num_pages: int) -> str:
        """Ensure PDF is searchable using OCR if needed."""
        try:
            # Check if PDF is already searchable
            doc = fitz.open(pdf_path)
            first_page = doc[0]
            text = first_page.get_text()
            doc.close()
            
            if text.strip():
                logging.info("PDF is already searchable")
                return pdf_path
            
            # OCR the PDF
            logging.info("PDF is not searchable, running OCR...")
            
            output_dir = os.path.dirname(pdf_path)
            ocr_output = os.path.join(output_dir, f"ocr_{os.path.basename(pdf_path)}")
            
            # Determine OCR pages based on document type
            if num_pages <= 50:
                # OCR all pages for short documents
                ocr_pages = None
            else:
                # OCR only first 10 pages for long documents
                ocr_pages = "1-10"
            
            cmd = ["ocrmypdf", "--deskew", "--force-ocr", "-l", "eng+chi_sim"]
            if ocr_pages:
                cmd.extend(["--pages", ocr_pages])
            cmd.extend([pdf_path, ocr_output])
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                logging.info(f"OCR completed successfully: {ocr_output}")
                return ocr_output
            else:
                logging.error(f"OCR failed: {result.stderr}")
                return pdf_path
                
        except Exception as e:
            logging.error(f"Error ensuring searchable PDF: {e}")
            return pdf_path
    
    def _extract_with_llm(self, pdf_path: str, doc_type: str) -> Dict:
        """Extract citation using LLM with DSPy."""
        try:
            # Define signatures for different document types
            if doc_type == "book_or_thesis":
                signature = dspy.Signature(
                    "pdf_text -> title, author, publisher, year, location, editor, translator, volume, series, isbn, doi, thesis_type",
                    "Extract citation information from book/thesis PDF text. Focus on cover and copyright pages."
                )
            else:
                signature = dspy.Signature(
                    "pdf_text -> title, author, journal_name, year, number, page_numbers, book_name, publisher, editor, isbn, doi",
                    "Extract citation information from journal/chapter PDF text. Focus on first page header/footer."
                )
            
            # Extract text from PDF
            doc = fitz.open(pdf_path)
            
            if doc_type == "book_or_thesis":
                # Extract first 5 pages for books/thesis
                pages_to_extract = min(5, doc.page_count)
            else:
                # Extract first page for journals/chapters
                pages_to_extract = 1
            
            text = ""
            for i in range(pages_to_extract):
                page = doc[i]
                text += page.get_text() + "\n"
            
            doc.close()
            
            # Use LLM to extract citation
            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=text)
            
            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip():
                    citation_info[key] = value.strip()
            
            logging.info(f"LLM extraction result: {citation_info}")
            return citation_info
            
        except Exception as e:
            logging.error(f"Error with LLM extraction: {e}")
            return {}
    
    def _determine_url_type(self, url: str) -> str:
        """Determine URL type."""
        try:
            response = requests.head(url, timeout=10)
            content_type = response.headers.get('content-type', '').lower()
            
            if 'video' in content_type or 'audio' in content_type:
                return "media"
            else:
                return "text"
        except Exception as e:
            logging.error(f"Error determining URL type: {e}")
            return "text"
    
    def _extract_url_with_anystyle(self, url: str) -> Dict:
        """Extract citation from URL using anystyle."""
        try:
            # Use anystyle via subprocess
            result = subprocess.run(
                ['anystyle', 'parse', url],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                # Parse anystyle output (adjust based on actual format)
                citation_info = {}
                # This is a placeholder - you'll need to parse the actual anystyle output
                logging.info(f"Anystyle output: {result.stdout}")
                return citation_info
            else:
                logging.warning(f"Anystyle failed: {result.stderr}")
                return {}
        except Exception as e:
            logging.error(f"Error with anystyle: {e}")
            return {}
    
    def _extract_media_metadata(self, url: str) -> Dict:
        """Extract metadata from media URLs."""
        try:
            # Basic web scraping for metadata
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            citation_info = {}
            
            # Extract title
            title_tag = soup.find('title')
            if title_tag:
                citation_info['title'] = title_tag.get_text().strip()
            
            # Extract meta information
            meta_tags = soup.find_all('meta')
            for tag in meta_tags:
                name = tag.get('name', '').lower()
                content = tag.get('content', '')
                
                if name == 'author':
                    citation_info['author'] = content
                elif name == 'date':
                    citation_info['date'] = content
                elif name == 'publisher':
                    citation_info['publisher'] = content
            
            logging.info(f"Media metadata: {citation_info}")
            return citation_info
            
        except Exception as e:
            logging.error(f"Error extracting media metadata: {e}")
            return {}
    
    def _save_citation(self, citation_info: Dict, input_source: str, output_dir: str):
        """Save citation information as YAML and JSON."""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate base filename
            if input_source.startswith('http'):
                # URL
                base_name = "url_citation"
            else:
                # PDF file
                base_name = os.path.splitext(os.path.basename(input_source))[0]
            
            # Save as YAML
            yaml_path = os.path.join(output_dir, f"{base_name}.yaml")
            with open(yaml_path, 'w', encoding='utf-8') as f:
                yaml.dump(citation_info, f, default_flow_style=False, allow_unicode=True)
            
            # Save as JSON
            json_path = os.path.join(output_dir, f"{base_name}.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(citation_info, f, indent=2, ensure_ascii=False)
            
            logging.info(f"Citation saved to: {yaml_path} and {json_path}")
            
        except Exception as e:
            logging.error(f"Error saving citation: {e}")


# Legacy function for backward compatibility
def get_pdf_citation_text(input_pdf_path, output_dir="processed_pdfs", lang="eng"):
    """Legacy function for backward compatibility."""
    extractor = CitationExtractor()
    return extractor.extract_from_pdf(input_pdf_path, output_dir)
